\newpage
\chapter{Method}

%-----------------------------
% METHODS INTRODUCTION
% Remember to include:
% - What problem are you solving formally (inputs, outputs, assumptions)
% - How are you solving this problem
%-----------------------------
This project addresses the task of Visual Question Answering within 3D indoor environments (3D-VQA). 3D-VQA consists in answering open-vocabulary, free-form questions about an environment. For example, given segmented RGB-D images of a hallway and the question "Where did I put my keys?", the 3D-VQA model should be able to locate the most relevant object in the scene and output an accurate response to the question, such as "On top of the drawer at position ($x, y, z$)".

The two main challenges of the 3D-VQA task are 1) to create a rich scene representation, often starting from RGB-D frames and 2) to combine scene representations and text queries in a 3D-VQA model producing an accurate output answer. The 3D-VQA task hence requires generating and merging different data modalities in a multimodal model, which conditions output answers on the given question-scene input pairs.

While current research often claims to utilize scene graphs as scene representations, it does not utilize edge information in the 3D-VQA model, and solely relies on node information. This means that current methods do not leverage the full potential of scene graphs, and do not bake any relational or structural information of the scene into its representation. Furthermore, existing 3D-VQA models often rely solely on pre-trained language-vision models, such as CLIP, to bring the question and scene representation into the same embedding space, lacking a learning mechanism. 

To mitigate these limitations, we propose to develop:
\begin{enumerate}
    \item A dataset of scene graphs for the ScanNet dataset (ScanSG), paired with questions from the ScanQA dataset, regarding the graphs. This is, to our knowledge, the first dataset of scene graph-question pairs for 3D indoor environments.
    \item An end-to-end 3D-VQA classification model (ScanSG-GNN), which combines graph and text modalities to answer questions about the scene.
\end{enumerate}

It should be noted that the proposed VQA model is a classificaton model, rather than a generation model. Hence, the model does not aim to output free-form answers, but rather solves the task of locating the most relevant object to the question posed. For example, the question "What color is the chair by the desk" should return the relevant instance of "chair", rather than the answer "the chair is black".
In other words, the proposed model returns the object which answers the question "Where should one look to answer the question?", rather than a free-form answer to the question.

This classification task is easier to evaluate and interpret than the generation task, and hence a good starting point for evaluating the model's ability to understand question-scene pairs. Furthermore, successfully solving the classification task ensures that the proposed VQA model relies on structural scene understanding rather than textual priors, since a classification model must focus on instance accuracy rather than semantic accuracy. In other words, solving the classification task rather than the generation task forces the model to learn to understand the scene and the question, rather than to learn common sense. Our model can be easily extended to a generation model by adding a decoder component to the model output.

The following sections detail the scene graph generation method used to obtain the ScanSG dataset, as well as the proposed 3D-VQA classification models.

\textcolor{red}{+ in ScanQA paper, it is shown that text generation works MUCH better when it is trained alongside classification. This shows that the classification task is actually super important and the better we can perfect it/understand it, the more likely we are to develop a good generation model using that.} 

\textcolor{red}{+ what makes this task hard is that its basically zero-shot classification!}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Scene Graph Generation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scene Graph Generation}

\textcolor{red}{[Add figure here w each step summarized, + examples of output scene graph (can also add to appendix)?]}

We aim produce semantically meaningful graph representations of the ScanNet indoor scenes. The scene representation should simplify the point-cloud representation into a scene graph, while preserving 3D structural information. The 3D structural information differentiates scenes with the same objects placed differently, and hence enables reasoning over the space.

To achieve this, we let graph nodes represent segmented objects in the scene, and graph edges represent proximity, such that nearby nodes are connected by an edge. We note that there is no such "ground truth" representation, since scene graph generation requires some design choices. More specifically, to construct a graph representation $G = (V, A, E)$, the following components must be defined:

\begin{enumerate}
    \item \textbf{Node features, $V \in \mathbb{R}^{n \times D_V}$: } the graph contains $n$ nodes of $D$-dimensional embedding, which encodes information about the object it represents.
    \item \textbf{Adjacency, $A \in \mathbb{R}^{n \times n}$:} the graph edges are defined in an $n \times n$ adjacency matrix, which encodes node connectivity. We note that node connectivity can be chosen to represent any type of similarity, such as proximity, visual similarity, semantic similarity or functional similarity.
    \item \textbf{Edge features, $E \in \mathbb{R}^{n \times n \times D_E} $: } Optionally, $D_E$-dimensional edge features $E$ can also be defined. Edge features could for example encode the relation between nodes, such as "next to", "inside", "above", "same color", or "same function".
\end{enumerate}

The following sections detail the heuristics-based method used in this paper to generate ScanSG. We first describe the node feature generation, then the adjacency generation and finally the edge feature generation. Together, these components form the ScanSG dataset.

\subsection{Node features}
While existing SGG methods commonly segment the scene as a first step (for an end-to-end method), we opt to use oracle segmentation provided by ScanNet to get a more accurate SG dataset.

We initialize the node embeddings with CLIP-embedded images of the object of interest. However, as each object may appear in many RGB-D frames under different views, we must select which views to embed (\textbf{View Selection}), and how to combine these into a single node embedding (\textbf{Cropping + Embedding}).

\textcolor{red}{+ CLIP embeds the appearance and semantic meaning of the object (color, shape, function, ...). This is great because if we provide a textual description of the object, then its CLIP embedding should be quite close to the image. (CLIP was trained on image-description pairs).}

\bigskip
\noindent
\textbf{View Selection}
Given the high frame rate of the image capture, we discard every other RGB-D image in each scene to speed up the data processing. We wish to filter out poor object views, and to only consider the top-$k$ views of each object. The top-$k$ views for each object are selected by ranking views based on their view score $s_f$, which we define as:

\begin{equation*}
    s_f = \frac{n_f}{w \times h} \times \frac{b_f}{8},
\end{equation*}

\noindent
where $n_f$ is the number of pixels occupied by the object in the frame $f$, $w$ and $h$ are the width and height of the frame and $b_f$ is the number of bounding box corners visible in the frame (out of 8 possible corners). $b_f$ is obtained by projecting the 3D-coordinates of a tight, axis-aligned object bounding box (from the ScanNet oracle point-cloud segmentation) into each RGB-frame $f$. $b_f = 8$ implies that the entire object is visible in the frame, while $b_f < 8$ implies a partial view of the object.

We refer to the first term of $s_f$ as the \textit{pixel score}, since it represents how much space the object takes up in the frame (close-up views are preferred). The second term is the \textit{corner score}, which represents how much of the total object is visible in the frame (full views are preferred to partial views). A view score $s_f \approx 1$ implies a close-up view of a fully visible object, whereas $s_f \approx 0$ implies either a distant view, or a partial view, or both.

\bigskip
\textcolor{red}{[ multiplying vs. adding? if adding, We note that weights could be added to both the "size" and "visibility" components of the score to weigh their relative importance.]}

\bigskip
\noindent
\textbf{Cropping}
For each object, we then crop the top-$k$ views around the object of interest. In the ScanSG dataset, a square crop centred around the object, and resized to $224 \times 224$ is used. This is because CLIP was trained on this input size. Other cropping methods are explored and compared in section XX.

\bigskip
\noindent
\textbf{Embedding}
Pre-trained CLIP was chosen to embed images as it brings images and text into the same embedding space, allowing for comparison between the two.
For each object, the top-$k$ crops are then CLIP-embedded (using the Hugging Face implementation [XX]) and aggregated using the $s_f$-weighted average. This yields a single 512-dimensional CLIP-embedding for each object.
This aggregated embedding captures view variability, giving the object representation a richer embedding.

\subsection{Adjacency}
The adjacency matrix is the graph component encoding 3D information. Edges in the ScanSG dataset represent spatial proximity of nodes. In other words, an edge should connect two nodes if the distance between them is less than some predetermined heuristic threshold.

\textcolor{red}{+ The threshold method is justified because from scene XX, the object density is pretty consistent across scenes - so a single threshold value should work for all scenes.}

To define the adjacency matrix, the shortest distance between all objects in a scene should therefore be calculated.

The naive approach to this problem is to compute the distance between all 3D points of 2 object point-clouds, and to retain the shortest distance. Although this is most accurate method to estimate the shortest distance between two objecs, it is too computationally expensive, and also sensitive to outliers.

We therefore approximate the shortest distance between two objects by the shortest distance between their axis-aligned 3D bounding boxes. This can be calculated as:

\begin{equation*}
    d = ...
\end{equation*}

A threshold of 0.2 was selected heuristically. The adjacency matrix was symmetrized to ensure bidirectional relationships between nodes. Additionally, self-loops were added.


\subsection{Edge features}
We compare two learning-based approaches to annotate/predict the edge label of the edges determined above.

We hand-label 275 edges from the following prepositions list:
on/above, under
inside, contains
next to
attached to, supporting

However this dataset is strongly imbalanced (mostly next to). 

\bigskip
\noindent
\textbf{Random Forest}

\bigskip
\noindent
\textbf{MLP}






\section{VQA Models}
%-----------------------------
% VQA MODEL METHODS
%-----------------------------

[for models, add the number of total parameters]
[also add details about the GNN you use (GCN with 512 in, 512 out)].


Figure XX illustrates the general 3D-VQA classification method. VQA models take as inputs a question and a scene graph (generated as described above), and outputs the most relevant object in the scene to the question. Regardless of architecture, any VQA model must have the following components: 1) a scene encoder, 2) a question encoder, 3) object selection head, which combine scene and question encodings to select the output object.

We explore solutions for each of these components, and compare the performance of three distinct families of model architectures: a baseline model (no learning), a GNN-only (learning only for scene encoding) model, and a transformer-based model (learning for scene encoding, question encoding and selection head). Some of the components are shared across models, this helps us identify which components of the final proposed model are actually helpful.

The following sections detail the architecture and design choices of each model.

We incrementally build up the proposed model the baseline, adding complexity/degrees of freedom to the model at each step.


\subsection{Baseline}
The baseline model is used to establish a lower bound on the performance of the VQA task. The baseline model is illustrated in Figure XXX. It does not require any learning, and relies solely on the pre-computed CLIP embeddings of the objects in the scene and the question. The model consists of the following components:

\textbf{Scene Encoder:} The scenes are encoded as the set of its ScanNet-SG scene graph nodes (without edges, obtained as described in section XXX).

\textbf{Question Encoder:} The question encoding is the sentence-level CLIP embedding of the question.

\textbf{Object Selection Head:} The model computes the cosine similarity between the question embedding and the node embeddings of the scene graph. The node with the highest similarity is selected as the output object.

While this baseline model is a simple and common approach to the VQA task [ref], it has some important limitations. Firslty, the scene encoder encurs a large information loss: by discarding edges, it does not encode any 3D or structural information about the scene. It therefore does not capture the overall room layout, and lacks contextual awareness. This implies that objects with similar appearance may have indifferentiable encodings, regardless of their context and neighbouring objects, and that the overall scene encoding does not change if even if objects are moved around. For example, the baseline model may not be able to differentiate between a chair in the middle of the room and a chair in the corner of the room, as the scene representation would be the same. This means that while semantic accuracy might be high, instance accuracy is expected to be low. Furthermore, the model has no learning mechanism, meaning that the scene representation cannot adapt to new questions.

\subsection{Baseline + GNN}
To address these limitations, we explore the Baseline + GNN model, which uses a graph neural network to learn better node embeddings. The model is illustrated in Figure XXX. The model consists of the following components:

\textbf{Scene Encoder:} The ScanNet-SG scene graphs are passed through a dimension-preserving GNN. The GNN updates node embeddings iteratively by aggregating information from neighbouring nodes. This ensures each node becomes aware of its context.

- What type of GNN architecture? How many layers? How many hidden units? What activation function? What aggregation function? What loss function?

\textbf{Question Encoder:} Same as the baseline model.

\textbf{Object Selection Head:} Same as the baseline model.

The GNN-only model is expected to perform better than the baseline model, as structural information is added to the scene encoding. This model should therefore better differentiate between objects with similar appearance but different context. Furthermore, the scene encoder is learned, meaning that the model can learn to adapt scene encodings to better match the question encoding. In other words, the model can learn to align the question and scene embeddings in a learned way, rather than a fixed way as in the baseline model. However, the GNN-only model still has some limitations. Firstly, the model is still limited by the question encoding and object selection, which are not learned. This means that the model may struggle  with questions that are not well-aligned with the learned scene graph.

\subsection{ScanSG-GNN}

The transformer-based model incorporates learning for the question encoding and object selection head. It is illustrated in Figure XXX. The idea was to create a model with more degrees of freedom than the GNN-only model: instead of trying to align the fixed question embeddings and learned scene embeddings using cosine similarity, the model learns to predict a  score for each object instance in the scene based on the learned scene and question embeddings. High score indicates high relevance to the question.

The model is composed of four main components: 1) a question encoder, 2) a scene graph encoder, 3) a cross-attention mechanism, and 4) a classifier.

The question encoder is a transformer-based model which encodes the question into a fixed-size vector. The scene graph encoder is a graph neural network which encodes the scene graph into a fixed-size vector. The classifier is a multi-layer perceptron which takes the question and scene graph embeddings as input and outputs the most relevant object in the scene to the question.

How many layers? How many hidden units? What activation function? What aggregation function? What edge update function? What loss function?

\subsection{ScanSG-Transformer}

\subsection{Batching and Masking}

\subsection{Loss function}

\textbf{Cross-entropy loss}
\textbf{Multi-object Cross-entropy loss}
\textbf{Focal loss}